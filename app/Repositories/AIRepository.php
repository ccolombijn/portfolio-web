<?php

declare(strict_types=1);

namespace App\Repositories;

use Anthropic\Client as AnthropicClient;
use App\Contracts\AIRepositoryInterface;
use Gemini\Client as GeminiClient;
use Gemini\Data\Content;
use Gemini\Enums\Role;
use HelgeSverre\Mistral\Mistral as MistralClient;
use Illuminate\Http\JsonResponse;
use Illuminate\Support\Facades\Log;
use Illuminate\Support\Facades\Storage;
use InvalidArgumentException;
use OpenAI\Client as OpenAIClient;
use Spatie\PdfToText\Exceptions\BinaryNotFoundException;
use Spatie\PdfToText\Pdf;
use Symfony\Component\HttpFoundation\StreamedResponse;
use Throwable;

final class AIRepository implements AIRepositoryInterface
{
    public function __construct(
        private readonly OpenAIClient $openai,
        private readonly GeminiClient $gemini,
        private readonly AnthropicClient $anthropic,
        private readonly MistralClient $mistral,
    ) {}

    /**
     * Get available AI profile names.
     */
    public function getAvailableProfiles(): array
    {
        $storage = Storage::disk('public');
        $path = 'json/profiles';

        if (! $storage->exists($path)) {
            Log::warning("AI profiles directory not found: {$path}");

            return [];
        }

        $files = $storage->files($path);

        return collect($files)
            ->filter(fn($file) => pathinfo($file, PATHINFO_EXTENSION) === 'json')
            ->map(fn($file) => pathinfo($file, PATHINFO_FILENAME))
            ->values()
            ->all();
    }

    /**
     * Generate AI response based on the specified provider.
     */
    public function generate(array $data, ?string $provider = null): JsonResponse|StreamedResponse
    {
        $task = array_key_exists($data['prompt'], config('ai.prompts', [])) ? $data['prompt'] : 'chat';
        $taskConfig = $this->getTaskConfig($task);

        $provider = $taskConfig['provider'];
        // If a model is specified in the task config, inject it into the data array.
        if (!isset($data['model']) && $taskConfig['model']) {
            $data['model'] = $taskConfig['model'];
        }

        return match ($provider) {
            'openai' => $this->generateWithOpenAI($data),
            'gemini' => $this->generateWithGemini($data),
            'anthropic' => $this->generateWithAnthropic($data),
            'mistral' => $this->generateWithMistral($data),
            default => throw new InvalidArgumentException("Unsupported AI provider: [{$provider}]")
        };
    }

    /**
     * Handles generation using the OpenAI client.
     */
    private function generateWithOpenAI(array $data): JsonResponse|StreamedResponse
    {
        try {
            $model = $data['model'] ?? config('ai.models.openai', 'gpt-5-nano');
            $messages = $this->buildOpenAIMessages($data);

            if (! empty($data['stream'])) {
                return $this->streamOpenAIResponse($model, $messages);
            }

            return $this->generateResponse('openai', $model, $messages);
        } catch (Throwable $e) {
            return $this->handleErrorResponse($e, 'OpenAI');
        }
    }
    /**
     * Build OpenAI messages with given data.
     */
    private function buildOpenAIMessages(array $data): array
    {
        $messages = [];

        if (! empty($data['history'])) {
            foreach ($data['history'] as $item) {
                $role = $item['role'] ?? 'user';
                if (in_array($role, ['user', 'assistant', 'system'])) {
                    $messages[] = ['role' => $role, 'content' => (string) ($item['text'] ?? '')];
                }
            }
        }

        $prompt = $this->buildBasePrompt($data);

        $messages[] = ['role' => 'user', 'content' => $prompt];
        return $messages;
    }
    /**
     * Handles streaming generation using the OpenAI client.
     */
    private function streamOpenAIResponse(string $model, array $messages): StreamedResponse
    {
        $stream = $this->openai->chat()->createStreamed([
            'model' => $model,
            'messages' => $messages,
        ]);

        return $this->streamResponse($stream, fn($chunk) => $chunk->choices[0]->delta->content ?? null);
    }

    /**
     * Handles generation using the Gemini client.
     */
    private function generateWithGemini(array $data): JsonResponse|StreamedResponse
    {
        try {
            $model = $data['model'] ?? config('ai.models.gemini', 'gemini-2.5-flash');
            $prompt = $this->buildGeminiPrompt($data);

            if (! empty($data['stream'])) {
                return $this->streamGeminiResponse($model, $prompt);
            }
            // Non-streaming response
            return $this->generateResponse('gemini', $model, $prompt);
        } catch (Throwable $e) {
            return $this->handleErrorResponse($e, 'Gemini');
        }
    }
    /**
     * Handles streaming generation using the Gemini client.
     */
    private function streamGeminiResponse(string $model, $prompt): StreamedResponse
    {
        Log::debug('Gemini: Starting streaming response.', ['model' => $model, 'prompt' => $prompt]);
        $stream = is_array($prompt)
            ? $this->gemini->generativeModel($model)->streamGenerateContent(...$prompt)
            : $this->gemini->generativeModel($model)->streamGenerateContent($prompt);

        return $this->streamResponse($stream, fn($chunk) => $chunk->text());
    }

    /**
     * Build prompt with given data for Gemini.
     */
    private function buildGeminiPrompt(array $data): string|array
    {
        $basePromptParts = $this->buildBasePromptParts($data);

        if (isset($data['history'])) {
            $history = [];

            // Add system prompt and file context as initial user messages if not already in history
            if (!empty($basePromptParts['system_prompt'])) {
                $history[] = Content::parse(part: $basePromptParts['system_prompt'], role: Role::USER);
                $history[] = Content::parse(part: 'Ok, begrepen.', role: Role::MODEL); // Acknowledge the system prompt
            }
            if (!empty($basePromptParts['file_context'])) {
                $history[] = Content::parse(part: $basePromptParts['file_context'], role: Role::USER);
                $history[] = Content::parse(part: 'Ok, ik heb de bestanden gelezen.', role: Role::MODEL); // Acknowledge the file context
            }

            foreach ($data['history'] as $item) {
                $role = ($item['role'] ?? 'user') === 'model' ? Role::MODEL : Role::USER;
                $partText = (string) ($item['text'] ?? '');
                $history[] = Content::parse(part: $partText, role: $role);
            }

            return [...$history, Content::parse(part: $basePromptParts['prompt'], role: Role::USER)];
        }

        return trim(implode("\n\n", array_filter($basePromptParts)));
    }

    /**
     * Handles generation using the Anthropic client.
     */
    private function generateWithAnthropic(array $data): JsonResponse|StreamedResponse
    {
        try {
            $model = $data['model'] ?? config('ai.models.anthropic', 'claude-3-haiku-20240307');
            $messages = $this->buildAnthropicMessages($data);
            $systemPrompt = $this->buildBasePromptParts($data)['system_prompt'];

            if (! empty($data['stream'])) {
                return $this->streamAnthropicResponse($model, $messages, $systemPrompt);
            }

            return $this->generateResponse('anthropic', $model, $messages, $systemPrompt);
        } catch (Throwable $e) {
            return $this->handleErrorResponse($e, 'Anthropic');
        }
    }

    /**
     * Handles streaming generation using the Anthropic client.
     */
    private function streamAnthropicResponse(string $model, array $messages, string $systemPrompt): StreamedResponse
    {
        $stream = $this->anthropic->messages()->createStreamed([
            'model' => $model,
            'system' => $systemPrompt,
            'messages' => $messages,
            'max_tokens' => 4096,
        ]);

        return $this->streamResponse($stream, fn($chunk) => $chunk->type === 'content_block_delta' ? $chunk->delta->text : null);
    }

    private function buildAnthropicMessages(array $data): array
    {
        $allMessages = [];

        if (! empty($data['history'])) {
            foreach ($data['history'] as $item) {
                $role = $item['role'] ?? 'user';
                if (in_array($role, ['user', 'assistant'])) {
                    $allMessages[] = ['role' => $role, 'content' => (string) ($item['text'] ?? '')];
                }
            }
        }

        $baseParts = $this->buildBasePromptParts($data);
        $prompt = trim(implode("\n\n", array_filter([$baseParts['file_context'], $baseParts['prompt']])));

        $allMessages[] = ['role' => 'user', 'content' => $prompt];

        if (empty($allMessages)) {
            return [];
        }

        // Merge consecutive messages from the same role to comply with Anthropic's format.
        $messages = [];
        $lastMessage = array_shift($allMessages);

        foreach ($allMessages as $currentMessage) {
            if ($currentMessage['role'] === $lastMessage['role']) {
                $lastMessage['content'] .= "\n\n" . $currentMessage['content'];
            } else {
                $messages[] = $lastMessage;
                $lastMessage = $currentMessage;
            }
        }
        $messages[] = $lastMessage;

        return $messages;
    }

    /**
     * Handles generation using the Mistral client.
     */
    private function generateWithMistral(array $data): JsonResponse|StreamedResponse
    {
        try {
            $model = $data['model'] ?? config('ai.models.mistral', 'mistral-large-latest');
            $messages = $this->buildMistralMessages($data);

            if (! empty($data['stream'])) {
                return $this->streamMistralResponse($model, $messages);
            }

            return $this->generateResponse('mistral', $model, $messages);
        } catch (Throwable $e) {
            return $this->handleErrorResponse($e, 'Mistral');
        }
    }

    /**
     * Handles streaming generation using the Mistral client.
     */
    private function streamMistralResponse(string $model, array $messages): StreamedResponse
    {
        $stream = $this->mistral->chat()->createStreamed([
            'model' => $model,
            'messages' => $messages,
        ]);

        return $this->streamResponse($stream, fn($chunk) => $chunk->choices[0]->delta->content ?? null);
    }

    /**
     * Build Mistral messages with given data. This is identical to OpenAI's structure.
     */
    private function buildMistralMessages(array $data): array
    {
        // The helgesverre/mistral package uses the same message format as OpenAI's
        return $this->buildOpenAIMessages($data);
    }


    /**
     * Generic method to handle non-streaming responses from any provider.
     */
    private function generateResponse(string $provider, string $model, array|string $prompt, ?string $systemPrompt = null): JsonResponse
    {
        $responseText = match ($provider) {
            'openai' => $this->openai->chat()->create([
                'model' => $model,
                'messages' => $prompt,
            ])->choices[0]->message->content,

            'gemini' => $this->gemini->generativeModel($model)->generateContent($prompt)->text(),

            'anthropic' => $this->anthropic->messages()->create([
                'model' => $model,
                'system' => $systemPrompt,
                'messages' => $prompt,
                'max_tokens' => 4096,
            ])->content[0]->text,

            'mistral' => (function () use ($model, $prompt) {
                /** @var \HelgeSverre\Mistral\Responses\Chat\CreateResponse $response */
                $response = $this->mistral->chat()->create(['model' => $model, 'messages' => $prompt]);

                return $response->choices[0]->message->content;
            })(),

            default => throw new InvalidArgumentException("Unsupported AI provider for non-streaming generation: [{$provider}]"),
        };

        return response()->json(['response' => $responseText]);
    }


    /**
     * Generic method to handle streaming responses from any provider.
     */
    private function streamResponse(iterable $stream, callable $textExtractor): StreamedResponse
    {
        return new StreamedResponse(function () use ($stream, $textExtractor) {
            try {
                foreach ($stream as $chunk) {
                    $text = $textExtractor($chunk);
                    if (! empty($text)) {
                        echo $text;
                        if (ob_get_level() > 0) {
                            ob_flush();
                        }
                        flush();
                    }
                }
            } catch (Throwable $e) {
                Log::error('An unexpected error occurred during the stream.', ['exception' => $e]);
                echo "[ERROR: An unexpected error occurred during the stream.]";
                if (ob_get_level() > 0) {
                    ob_flush();
                }
                flush();
            }
        }, 200, [
            'Content-Type' => 'text/plain',
            'X-Accel-Buffering' => 'no',
            'Cache-Control' => 'no-cache',
        ]);
    }

    /**
     * Handles exceptions and returns a standardized JSON error response.
     */
    private function handleErrorResponse(Throwable $e, string $provider): JsonResponse
    {
        Log::error("Failed to get a response from {$provider}.", ['exception' => $e]);

        $message = config('app.debug')
            ? $e->getMessage()
            : "Failed to get a response from {$provider}. Please try again later.";

        return response()->json(['error' => $message], 500);
    }

    /**
     * Get the provider and model for a specific task from the configuration.
     *
     * @return array{provider: string, model: ?string}
     */
    private function getTaskConfig(string $task): array
    {
        $handler = config("ai.tasks.{$task}") ?? config('ai.default_handler');

        if (!str_contains($handler, ':')) {
            throw new InvalidArgumentException("Invalid AI handler format for task '{$task}'. Expected 'provider:model', but got '{$handler}'.");
        }

        [$provider, $model] = explode(':', $handler, 2);

        return ['provider' => $provider, 'model' => $model];
    }



    /**
     * Build the base prompt including file contexts.
     */
    private function buildBasePrompt(array $data): string
    {
        $parts = $this->buildBasePromptParts($data);

        return trim(implode("\n\n", array_filter($parts)));
    }
    /**
     * Build prompt with given data
     * @return array{system_prompt: string, file_context: string, prompt: string}
     */
    private function buildBasePromptParts(array $data): array
    {
        $profileName = $data['profile'] ?? config('ai.default_profile');
        $profileData = null;
        if ($profileName) {
            $profileData = $this->loadProfile($profileName);
        }

        $prompt = (string) ($data['prompt'] ?? '');
        $isPredefinedPrompt = array_key_exists($prompt, config('ai.prompts', []));

        // If the prompt is a key for a predefined prompt, get the full text.
        if ($isPredefinedPrompt) {
            $promptTemplate = config('ai.prompts.' . $prompt);
            $prompt = str_replace(':input', $data['input'] ?? '', $promptTemplate);
        }

        if (str_contains($prompt, ':history')) {
            $historyString = collect($data['history'] ?? [])->map(fn($item) => ($item['role'] ?? 'user') . ': ' . ($item['text'] ?? ''))->implode("\n");
            $prompt = str_replace(':history', $historyString, $prompt);
        }

        $fileContext = '';
        // Only add file context if it's not a predefined prompt like 'explanation' or 'summarize'
        // We check the original prompt key here, not the full text.
        if (! $isPredefinedPrompt) {
            $defaultFiles = $profileData['files'] ?? config('ai.default_files', []);
            $requestFiles = $data['file_paths'] ?? [];
            $allFiles = array_unique(array_merge($defaultFiles, $requestFiles));

            foreach ($allFiles as $filePath) {
                $fileContent = $this->getFileContentForPrompt($filePath);
                if (null !== $fileContent) {
                    $fileName = basename($filePath);
                    $fileContext .= "File: `{$fileName}`\n\n```\n{$fileContent}\n```\n\n";
                }
            }
        }

        return [
            'system_prompt' => $data['system_prompt'] ?? $profileData['system_prompt'] ?? config('ai.prompts.system_prompt', ''),
            'file_context' => $fileContext,
            'prompt' => $prompt,
        ];
    }
    /**
     * Load AI profile from JSON file.
     * @param string $profileName
     */
    private function loadProfile(string $profileName): ?array
    {
        // Sanitize profile name to prevent directory traversal
        $profileName = basename($profileName);
        $path = "json/profiles/{$profileName}.json";
        $storage = Storage::disk('public');

        if (! $storage->exists($path)) {
            Log::warning("AI profile not found: {$path}");
            return null;
        }

        try {
            $content = $storage->get($path);
            return json_decode($content, true, 512, JSON_THROW_ON_ERROR);
        } catch (\JsonException $e) {
            Log::error("Failed to parse AI profile: {$path}", ['exception' => $e]);
            return null;
        }
    }

    // Sanitize and restrict path to storage/app/public
    private function getFileContentForPrompt(string $filePath): ?string
    {
        // Sanitize and restrict path to storage/app/public
        $filePath = str_replace('..', '', $filePath);

        $storage = Storage::disk('public');

        if (! $storage->exists($filePath)) {
            Log::warning('File path could not be found in storage/app/public.', ['path' => $filePath]);
            return null;
        }

        $fullPath = $storage->path($filePath);
        $realPath = realpath($fullPath);

        if (! $realPath || ! str_starts_with($realPath, realpath($storage->path('')))) {
            Log::warning('File path access attempt outside of storage/app/public.', ['path' => $filePath]);
            return null;
        }

        $extension = strtolower(pathinfo($filePath, PATHINFO_EXTENSION));

        if ($extension === 'pdf') {
            try {
                return Pdf::getText($fullPath);
            } catch (BinaryNotFoundException $e) {
                Log::critical('pdftotext binary not found. Please install poppler-utils on your system.', ['exception' => $e]);
                return null;
            } catch (Throwable $e) {
                Log::error("Failed to extract text from PDF: {$fullPath}", ['exception' => $e]);
                return "Error: Could not extract text from PDF file '{$filePath}'.";
            }
        }

        return $storage->get($filePath);
    }

    /**
     * Get text response from the specified provider.
     */
    private function getTextResponse(string $provider, string $model, string $prompt): string
    {
        return match ($provider) {
            'openai' => $this->openai->chat()->create([
                'model' => $model,
                'messages' => [['role' => 'user', 'content' => $prompt]],
            ])->choices[0]->message->content,
            'gemini' => $this->gemini->generativeModel($model)->generateContent($prompt)->text(),
            'anthropic' => $this->anthropic->messages()->create([
                'model' => $model,
                'messages' => [['role' => 'user', 'content' => $prompt]],
                'max_tokens' => 1024,
            ])->content[0]->text,
            'mistral' => (function () use ($model, $prompt) {
                /** @var \HelgeSverre\Mistral\Responses\Chat\CreateResponse $response */
                $response = $this->mistral->chat()->create(['model' => $model, 'messages' => [['role' => 'user', 'content' => $prompt]]]);

                return $response->choices[0]->message->content;
            })(),
            default => throw new InvalidArgumentException("Unsupported AI provider for prompt suggestions: [{$provider}]"),
        };
    }
    /**
     * Generate prompt suggestions based on the chat context.
     */
    public function suggestPrompts(array $data): array
    {
        try {
            $taskConfig = $this->getTaskConfig('suggest');
            $provider = $taskConfig['provider'];
            $model = $taskConfig['model'];

            $promptForSuggestions = $this->buildBasePrompt($data);

            $responseText = $this->getTextResponse($provider, $model, $promptForSuggestions);

            $suggestionsJson = preg_replace('/^```json\s*|\s*```$/', '', $responseText);

            $decoded = json_decode(trim($suggestionsJson), true);

            if (json_last_error() !== JSON_ERROR_NONE || !isset($decoded['suggestions']) || !is_array($decoded['suggestions'])) {
                Log::warning('AI prompt suggestion returned invalid JSON.', ['response' => $suggestionsJson]);
                return [];
            }

            return $decoded['suggestions'];
        } catch (Throwable $e) {
            Log::error('Failed to get prompt suggestions from AI.', ['exception' => $e]);
            throw $e;
        }
    }
}
